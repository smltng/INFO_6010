{
 "metadata": {
  "name": "",
  "signature": "sha256:654eac3d792368e108e4f690fd5dfb16d9dd04a8f3f3cd23590dff8a62c2f5fd"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "New York Times and the President (Round 2)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The past 6 years have been a roller coaster, with threats of global warming, H1N1, and ebola. Wars have broken out, and many tragedies have occurred. Of course, positive things have occurred over this time period as well, ranging from new reforms being passed in the United States to comeback of the automotive industry. As we near the end of President Obama's term, a big question starts to form: \"How well did he do?\". The way I choose to tackle this is to look at how positive or negative a popular newspaper, <i>The New York Times</i>, is in their overall stories about him."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To keep all the imports under control, I'm putting them in one place so it's easier to keep track. One thing to note is that I'm importing mini_liwc which DanCo wrote (it needs to be in the same directory if you want to run this code). Aside from that, the code at this point is pretty much the same as the last homework assignment: pull articles from the API on Obama, and convert the pulled articles into a tokenized form for analysis. Due to the nature of the \"liwc dictionaries\" I am using later, I chose not to stem my words. (Using stopwords is out of the question, especially if those words happen to carry positive/negative connotations.) "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nytimesarticle import articleAPI\n",
      "from collections import defaultdict\n",
      "from time import sleep\n",
      "from pprint import pprint\n",
      "import nltk\n",
      "import numpy\n",
      "import mini_liwc\n",
      "import csv\n",
      "import re"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#API key goes here\n",
      "api = articleAPI('xxxxxxxxxxxxxxxxxxxxxxxxxxxxx')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "articles = []\n",
      "\n",
      "#pull the 19 pages worth of data on obama\n",
      "for i in numpy.arange(0,19):\n",
      "    articles.append(api.search( q = 'Obama', fq = {'headline':'Obama', 'source':['Reuters','AP', 'The New York Times']}, begin_date = 20090120, page = i, sort = 'oldest', facet_field = ['source','day_of_week'], facet_filter = True ))\n",
      "    sleep(1) #pause for one second to give the server breathing room"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#add all the words to one big string\n",
      "entries = unicode(\"\")\n",
      "for article in articles:\n",
      "    for docs in article['response']:\n",
      "        for entry in article['response'][docs]:\n",
      "            if type(entry) is dict:\n",
      "                #compile the snippets into a list\n",
      "                entries = unicode(entries) + unicode(\" \") + unicode(entry['lead_paragraph'])\n",
      "\n",
      "tokens = [re.sub(\"[^0-9a-zA-Z]+\",\"\",word) for word in entries.split()]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have our data, we need to set up the liwc-style text analysis. To do this, we need to create a dictionary of positive and negative words. I looked online and found a word here: http://www3.nd.edu/~mcdonald/Word_Lists.html\n",
      "\n",
      "Grabbing and putting the data in a usable format was pretty simple. All I had to do was read the .csv file containing the words and put them into a dictionary in the format that the count_cats would accept."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#from lecture 2/9/15\n",
      "liwc = defaultdict(dict)\n",
      "\n",
      "\n",
      "with open('positive_words.csv','rU') as csvfile:\n",
      "    wordlist = csv.reader(csvfile)\n",
      "    for words in wordlist:\n",
      "        liwc[\"positive\"][words[0].lower()]=1 #words is a list always containing one element: an uppercase word\n",
      "with open('negative_words.csv','rU') as csvfile:\n",
      "    wordlist = csv.reader(csvfile)\n",
      "    for words in wordlist:\n",
      "        liwc[\"negative\"][words[0].lower()]=1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#just out of curiosity, how many positive and negative words do I have?\n",
      "print len(liwc[\"positive\"]),len(liwc[\"negative\"])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "354 2329\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results = mini_liwc.count_cats(liwc,tokens)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pprint(results)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "defaultdict(<type 'int'>, {'positive': 0.005593648502345724, 'negative': 0.020209310718152292})\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "At first glance, two things are apparent. First, the negative is much greater than the positive, which would be an indication that more negativity is being reported when it comes to Obama. Granted, the results returned are pretty low, barely reaching .02 for negative, which might be explained by the fact that there are a lot of tokens without positivity and negativity attached, like \"obama\" and \"barack\". Still, I decided to try for a bigger word database to see if that changed anything, especially with <i>The New York Times</i> being known for its democratic slant.\n",
      "\n",
      "Dataset can be found here: http://mpqa.cs.pitt.edu/lexicons/subj_sense_annotations/"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('subjectivity.tff','r') as f:\n",
      "    other_words = f.read()\n",
      "other_words_list = [word1 for word1 in other_words.split(\"\\n\")]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Parsing the data is a lot harder for this dataset than the other one, since it's all one string. I turn to the power of regular expressions to solve the problem."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# a sample string looks like this 'type=weaksubj len=1 word1=abandonment pos1=noun stemmed1=n priorpolarity=negative'\n",
      "new_dict = defaultdict(dict)\n",
      "\n",
      "import re\n",
      "for line in other_words_list:\n",
      "    #there are some unaccounted for things that needs to be cleaned\n",
      "    line = re.sub(\" mpqapolarity.*$\",\"\",line)\n",
      "    line = re.sub(\"weakneg\",\"negative\",line)\n",
      "    \n",
      "    #extract the polarity\n",
      "    polarity = re.sub(\"^.+priorpolarity=\",\"\",line) #polarity is at the end of the string, so cut out everything in the beginning\n",
      "    #extract the word\n",
      "    word = re.sub(\" pos1.*$\",\"\",re.sub(\"^.+word1=\",\"\",line))\n",
      "    \n",
      "    #add them to a dictionary of dictionaries with key polarity and word\n",
      "    if len(polarity)!=0:\n",
      "        new_dict[polarity][word] = 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#let's see how big the dataset is for this\n",
      "print len(new_dict[\"positive\"]), len(new_dict[\"negative\"])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2304 4154\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print mini_liwc.count_cats(new_dict,tokens)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "defaultdict(<type 'int'>, {'positive': 0.03265968964272826, 'neutral': 0.018224467701190905, 'negative': 0.025261638397690366, 'both': 0.00036088054853843375})\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "-----------------------------------\n",
      "\n",
      "The first thing to notice about the results from this run is how positivity seems to be more prevalent. Compared to the last run through, the negativity score does not appear to change too much, but there is a marked improvement in the positivity score that pulls it ahead. As the two results differ, however, it is important to decide which one is more trustworthy. In the case of the second attempt, there is much more robustness to the data, as we see many more words are present, not only boosting positivity and negativity scores, but also adding dimensions such as neutrality to words. As such, having this quality of data makes me inclined to believe that training the model on the second data set provides a more accurate result."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}